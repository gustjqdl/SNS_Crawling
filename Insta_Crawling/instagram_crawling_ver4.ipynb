{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver as wd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import ActionChains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스타그램 로그인 함수 \n",
    "\n",
    "def instagram_start():\n",
    "    options = wd.ChromeOptions()\n",
    "    options.add_argument('headless')\n",
    "\n",
    "\n",
    "    user_id = \"***\"\n",
    "    user_passwd = \"***\"\n",
    "    \n",
    "    \n",
    "    # 크롬 드라이버 경로 \n",
    "    driver_path = \"*****\"\n",
    "\n",
    "    url = 'https://www.instagram.com/accounts/login/'\n",
    "    facebook_login_page_css = \".sqdOP.yWX7d.y3zKF \"\n",
    "    facebook_id_form_id = \"email\"\n",
    "    facebook_pw_form_id = \"pass\"\n",
    "    facebook_login_btn_css = \"loginbutton\"\n",
    "\n",
    "    facebook_id_form_name = 'email'\n",
    "    facebook_pw_form_name = 'pass'\n",
    "    driver = wd.Chrome(driver_path)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "    driver.implicitly_wait(100)\n",
    "\n",
    "    id_box = driver.find_element_by_css_selector(\"#loginForm > div > div:nth-child(1) > div > label > input\")\n",
    "    password_box = driver.find_element_by_css_selector(\"#loginForm > div > div:nth-child(2) > div > label > input\")\n",
    "    login_button = driver.find_element_by_css_selector('#loginForm > div > div:nth-child(3) > button')\n",
    "\n",
    "    act = ActionChains(driver)\n",
    "    \n",
    "\n",
    "    act.send_keys_to_element(id_box,'****').send_keys_to_element(password_box,'****').click(login_button).perform()\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    return driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀레니움 컨트롤을 위한 CSS 좌표 값 \n",
    "\n",
    "main_text_object_css=\"div.C7I1f.X7jCj > div.C4VMK > span\" \n",
    "date_object_css=\"div.k_Q0X.NnvRN > a.c-Yi7 > time._1o9PC.Nzb55\"\n",
    "next_arrow_btn_css1=\"body > div._2dDPU.QPGbb.CkGkG > div.EfHg9 > div > div > div.l8mY4 > button\" \n",
    "next_arrow_btn_css2=\"body > div._2dDPU.QPGbb.CkGkG > div.EfHg9 > div > div > div.l8mY4 > button\" \n",
    "date_object_css=\"div.k_Q0X.NnvRN > a.c-Yi7 > time._1o9PC.Nzb55\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keywords 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords1 = ['원하시는 키워드를 입력하세요']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일일 수집 개수 설정 \n",
    "\n",
    "wish_num = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스타그램 피드 수집 함수 \n",
    "# Num에 입력된 숫자만큼 for문을 돌면서 피드 시간 / 내용 / URL을 수집합니다.\n",
    "# Return 값은 insta_df의 dict 형식으로 각 해시태그 별 내용을 Key와 Value 형식으로 갖습니다.\n",
    "\n",
    "def num_insta(num):\n",
    "    global driver \n",
    "    insta_df = dict()\n",
    "    \n",
    "    urls = []\n",
    "    date_texts = [] \n",
    "    date_times = [] \n",
    "    date_titles = [] \n",
    "    main_texts = [] \n",
    "    \n",
    "    check_arrow = True \n",
    "    \n",
    "    \n",
    "    global wish_num \n",
    "    count_extract = 0 \n",
    "\n",
    " \n",
    "    for k in range(num):\n",
    "        \n",
    "        if check_arrow is False : \n",
    "\n",
    "            \n",
    "            insta_df['date_text'] = date_texts\n",
    "            insta_df['date_time'] = date_times\n",
    "            insta_df['date_title'] = date_titles\n",
    "            insta_df['main_text'] = main_texts\n",
    "            insta_df['urls'] = urls\n",
    "\n",
    "            return insta_df\n",
    "            break \n",
    "        '''\n",
    "        if count_extract >= wish_num: \n",
    "            break \n",
    "        '''\n",
    "        time.sleep(1.0) \n",
    "        \n",
    "        # url & 날짜 \n",
    "       \n",
    "        try: \n",
    "            # url \n",
    "            cu = driver.current_url\n",
    "            urls.append(cu)\n",
    "            #날짜\n",
    "            date_object = driver.find_element_by_css_selector(date_object_css) \n",
    "            date_text = date_object.text \n",
    "            date_time = date_object.get_attribute(\"datetime\") \n",
    "            date_title = date_object.get_attribute(\"title\") \n",
    "\n",
    "\n",
    "        except: \n",
    "            urls.append('None')\n",
    "            date_text = None \n",
    "            date_time = None \n",
    "            date_title = None \n",
    "\n",
    "        try: \n",
    "            # 피드 내용 \n",
    "            main_text_object = driver.find_element_by_css_selector(main_text_object_css) \n",
    "            main_text = main_text_object.text\n",
    "            if main_text is None : \n",
    "                main_text = None\n",
    "\n",
    "        except: \n",
    "            main_text = None\n",
    "\n",
    "\n",
    "        date_texts.append(date_text) \n",
    "        date_times.append(date_time) \n",
    "        date_titles.append(date_title) \n",
    "        main_texts.append(main_text) \n",
    "        \n",
    " \n",
    "\n",
    "        try: \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html)\n",
    "            soup.select('.l8mY4.feth3')[0]\n",
    "            \n",
    "            WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.CSS_SELECTOR, next_arrow_btn_css1))) \n",
    "            driver.find_element_by_css_selector(next_arrow_btn_css2).click() \n",
    "\n",
    "        except: \n",
    "            check_arrow = False\n",
    "\n",
    "        count_extract +=1   \n",
    "\n",
    "\n",
    "       \n",
    "      \n",
    "\n",
    "        import numpy as np\n",
    "        MAX_SLEEP_TIME = 10\n",
    "        rand_value = np.random.randint(1, MAX_SLEEP_TIME)\n",
    "        time.sleep(rand_value)\n",
    "\n",
    "        #qwe= pd.DataFrame(data={'data':urls})['data']    \n",
    "        #insta_info_df = pd.DataFrame({\"date_text\":date_texts, \"date_time\":date_times, \"date_title\":date_titles,\"main_text\":main_texts, 'urls':qwe}) \n",
    "        #insta_info_df['키워드'] = keywords[j]\n",
    "        #insta_df = insta_df.append(insta_info_df)   \n",
    "        #print(insta_df)\n",
    "        \n",
    "\n",
    "    \n",
    "    insta_df['date_text'] = date_texts\n",
    "    insta_df['date_time'] = date_times\n",
    "    insta_df['date_title'] = date_titles\n",
    "    insta_df['main_text'] = main_texts\n",
    "    insta_df['urls'] = urls\n",
    "    \n",
    "   \n",
    "    return insta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드별 피드 리스트를 불러오는 함수 \n",
    "# keywords 리스트의 각 keyword를 돌면서 데이터를 수집합니다.\n",
    "\n",
    "# keyword별 게시글 수가 wish_num(수집하고자하는 개수) 보다 작으면 게시글 수만큼만 수집 \n",
    "# wish_num보다 크면 wish_num만큼만 수집\n",
    "\n",
    "# 수집하지 못한 keyword 또한 따로 df로 만들어 return합니다. \n",
    "\n",
    "def keyword_insta(keywords):\n",
    "    global driver \n",
    "    not_crawling= pd.DataFrame(columns={'url','keyword'})\n",
    "    instagram= pd.DataFrame()\n",
    "    \n",
    "    urls_lis= []\n",
    "    \n",
    "    global wish_num\n",
    "    for i in keywords : \n",
    "        url = \"https://www.instagram.com/explore/tags/{}/\".format(i) \n",
    "        urls_lis.append(url)\n",
    "        \n",
    "        \n",
    "    \n",
    "    for j in range(len(urls_lis)):\n",
    "        tmp = pd.DataFrame()\n",
    "        posting_cnts = [] \n",
    "        driver.get(urls_lis[j]) \n",
    "        \n",
    "        # HTML이 다 로드 될 때 까지 시간 초를 둡니다. \n",
    "        # 암시적 대기로 바꾸셔도 될 것 같습니다. \n",
    "        time.sleep(10)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        \n",
    "        \n",
    "        try : \n",
    "            \n",
    "            posting_cnt = soup.select('.g47SY')[0].text\n",
    "            posting_cnts = int(posting_cnt.replace(',', ''))\n",
    "            \n",
    "            count_extract = 0\n",
    "            time.sleep(2)\n",
    "  \n",
    "            \n",
    "            \n",
    "            #첫번째 게시물 클릭 \n",
    "            first_img_css=\"div.v1Nh3.kIKUG._bz0w\" \n",
    "            driver.find_element_by_css_selector(first_img_css).click()\n",
    "            \n",
    "                \n",
    "\n",
    "            if wish_num >= posting_cnts : \n",
    "\n",
    "                answer = num_insta(posting_cnts)\n",
    "\n",
    "                tmp['date_text'] = answer['date_text']\n",
    "                tmp['date_time'] = answer['date_time']\n",
    "                tmp['date_title'] = answer['date_title']\n",
    "                tmp['main_text'] = answer['main_text']\n",
    "                tmp['urls'] = answer['urls']\n",
    "                tmp['keyword'] = keywords[j]\n",
    "\n",
    "                instagram = instagram.append(tmp)\n",
    "                \n",
    "            else: \n",
    "   \n",
    "                answer = num_insta(wish_num)\n",
    "           \n",
    "                tmp['date_text'] = answer['date_text']       \n",
    "                tmp['date_time'] = answer['date_time']          \n",
    "                tmp['date_title'] = answer['date_title']            \n",
    "                tmp['main_text'] = answer['main_text']\n",
    "                tmp['urls'] = answer['urls']\n",
    "                tmp['keyword'] = keywords[j]\n",
    "            \n",
    "                instagram = instagram.append(tmp)\n",
    "              \n",
    "          \n",
    "           \n",
    "        \n",
    "        except :\n",
    "            print('다음 URL은 크롤링 할 수 없습니다.'+ urls_lis[j] )\n",
    "            lis = [keywords[j]] + [urls_lis[j]]\n",
    "            not_crawling = not_crawling.append(pd.Series(lis, index=not_crawling.columns), ignore_index=True)\n",
    "    \n",
    "            pass\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return instagram, not_crawling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = instagram_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instagram1, not_crawling1 = keyword_insta(keywords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "instagram = instagram1\n",
    "not_crawling = not_crawling1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "instagram = instagram[['date_title', 'main_text', 'urls','keyword']]\n",
    "not_crawling['개수'] = '<10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2021009\\.conda\\envs\\tf15_py37\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# keywords에 대한 카테고리 컬럼 추가\n",
    "\n",
    "instagram['category']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요청 부서별 분리 (예시)\n",
    "# list1은 A 부서에서 요청한 키워드 \n",
    "# list2은 B 부서에서 요청한 키워드 \n",
    "\n",
    "list1 = ['keyword']\n",
    "list2 = ['keyword']\n",
    "list3 = ['keyword']\n",
    "list4 = ['keyword']\n",
    "list5 = ['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2021009\\.conda\\envs\\tf15_py37\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(instagram['keyword'].tolist())):\n",
    "    if instagram.iloc[i,3]   in list1 : \n",
    "        instagram.iloc[i,4] = 'category1'\n",
    "        \n",
    "    elif instagram.iloc[i,3]  in list2 : \n",
    "        instagram.iloc[i,4] = 'category2'\n",
    "        \n",
    "    elif instagram.iloc[i,3]  in list3 : \n",
    "        instagram.iloc[i,4] = 'category3'\n",
    "        \n",
    "    elif instagram.iloc[i,3]  in list4 : \n",
    "        instagram.iloc[i,4] = 'category4'\n",
    "        \n",
    "    elif instagram.iloc[i,3]  in list5 : \n",
    "        instagram.iloc[i,4] = 'category5'\n",
    "    \n",
    "    else: \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# brand_keyword가 main_text에 포함된 행만 추출 \n",
    "# 예: 나이키 직원의 경우 신발 해시태그 중 나이키라는 단어가 언급된 피드에 관심이 있기 때문 (경쟁사 제외)\n",
    "\n",
    "instagram = instagram.query('main_text.str.contains(\"brand_keyword\")', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빅쿼리 데이터 불러와서 적재하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L 빅쿼리에 쌓기 API 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydata_google_auth\n",
    "credentials = pydata_google_auth.get_user_credentials(\n",
    "        ['https://www.googleapis.com/auth/bigquery'],)\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(project = '프로젝트 ID ', credentials= credentials)\n",
    "df_result=pd.DataFrame()\n",
    "query = \"select date_title,main_text, urls, keyword,category from instagram.total_instagram\"\n",
    "\n",
    "total_campaign = df_result.append(client.query(query).to_dataframe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_insta = total_campaign.append([instagram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_insta = tmp_insta.drop_duplicates(['date_title','urls','main_text','keyword','category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()\n",
    "df1 = df1.append([tmp_insta[tmp_insta['category']=='category1']])\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "df2 = df2.append([tmp_insta[tmp_insta['category']=='category2']])\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "df3 = df3.append([tmp_insta[tmp_insta['category']=='category3']])\n",
    "\n",
    "df4 = pd.DataFrame()\n",
    "df4 = df4.append([tmp_insta[tmp_insta['category']=='category4']])\n",
    "\n",
    "df5 = pd.DataFrame()\n",
    "df5 = df5.append([tmp_insta[tmp_insta['category']=='category5']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빅쿼리로 옮기기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_crawling.columns=['keyword','url','cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.76s/it]\n",
      "1it [00:04,  4.70s/it]\n",
      "1it [00:03,  3.80s/it]\n",
      "1it [01:10, 70.42s/it]\n",
      "1it [00:03,  3.41s/it]\n",
      "1it [00:05,  5.74s/it]\n",
      "1it [00:03,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "migration Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.oauth2 import service_account \n",
    "import pandas_gbq\n",
    "\n",
    "cd = service_account.Credentials.from_service_account_file(\n",
    "    'json 파일 위치 경로')\n",
    "\n",
    "project_id = '프로젝트ID '\n",
    "\n",
    "destination_table1 = 'instagram.total_instagram'\n",
    "destination_table2 ='instagram.category1'\n",
    "destination_table3 ='instagram.category2'\n",
    "destination_table4 ='instagram.category3'\n",
    "destination_table5 ='instagram.category4'\n",
    "destination_table6 ='instagram.category5'\n",
    "destination_table7 ='instagram.category6'\n",
    "\n",
    "tmp_insta.to_gbq(destination_table1, project_id, if_exists='replace', credentials=cd)\n",
    "df4.to_gbq(destination_table2, project_id, if_exists='replace', credentials=cd)\n",
    "df3.to_gbq(destination_table3, project_id, if_exists='replace', credentials=cd)\n",
    "df2.to_gbq(destination_table4, project_id, if_exists='replace', credentials=cd)\n",
    "df5.to_gbq(destination_table5, project_id, if_exists='replace', credentials=cd)\n",
    "df1.to_gbq(destination_table6, project_id, if_exists='replace', credentials=cd)\n",
    "not_crawling.to_gbq(destination_table7, project_id, if_exists='replace', credentials=cd)\n",
    "print('migration Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
